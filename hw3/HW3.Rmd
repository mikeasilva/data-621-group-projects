---
title: "DATA 621 Homework #3"
author: "Critical Thinking Group 3"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    highlight: kate
    toc_depth: 3
    code_folding: "hide"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment=NA, message=FALSE, warning=FALSE)
library(tidyverse)
library(kableExtra)
library(corrplot)
library(caret)
library(rpart)
library(rpart.plot)
library(Amelia)
# Thank you Stack Overflow!
# A Prefix nulling hook.

# Make sure to keep the default for normal processing.
default_output_hook <- knitr::knit_hooks$get("output")

# Output hooks handle normal R console output.
knitr::knit_hooks$set( output = function(x, options) {

  comment <- knitr::opts_current$get("comment")
  if( is.na(comment) ) comment <- ""
  can_null <- grepl( paste0( comment, "\\s*\\[\\d?\\]" ),
                     x, perl = TRUE)
  do_null <- isTRUE( knitr::opts_current$get("null_prefix") )
  if( can_null && do_null ) {
    # By default R print output aligns at the right brace.
    align_index <- regexpr( "\\]", x )[1] - 1
    # Two cases: start or newline
    re <- paste0( "^.{", align_index, "}\\]")
    rep <- comment
    x <- gsub( re, rep,  x )
    re <- paste0( "\\\n.{", align_index, "}\\]")
    rep <- paste0( "\n", comment )
    x <- gsub( re, rep,  x )
  }

  default_output_hook( x, options )

})
knitr::opts_template$set("kill_prefix"=list(comment=NA, null_prefix=TRUE))
```

```{r}
df <- read.csv('data/crime-training-data_modified.csv')
evaluation <- read.csv("data/crime-evaluation-data_modified.csv")
```

## Data Exploration

### Are There Missing Values?

```{r}
missmap(df, main = "Missing vs Observed Values")
```

It looks like we have a complete data set.  No need to impute values.

### Splitting the Data

```{r}
set.seed(42)
train_index <- createDataPartition(df$target, p = .7, list = FALSE, times = 1)
train <- df[train_index,]
test <- df[-train_index,]
```


### Exploratory Data Analysis


```{r}
train %>% 
  cor(.) %>%
  corrplot(., method = "color", type = "upper", tl.col = "black", diag = FALSE)
```

The following show the how predictors are distributed between the areas with crime rates higher than the median *blue) and below the median (red).  What we are looking for is variables that could split data into two groups.

```{r}
for (var in names(train)){
  if(var != "target"){
    plot_df <- train
    plot_df$x <- plot_df[,var]
    p <- ggplot(plot_df, aes(x, color = factor(target))) +
      geom_density() +
      theme_light() +
      ggtitle(var) +
      scale_color_brewer(palette = "Set1") +
      theme(legend.position = "none",
            axis.title.y = element_blank(),
            axis.title.x = element_blank())
    print(p)
  }
}
```

NOX seems to be the best variable to divide the data into the two groups

## Data Preparation
```{r}
ggplot(train, aes(rad , nox, color=as.factor(target))) +
  geom_point()
```


```{r}
library(FFTrees)

test <- test[names(train)]

fit <- FFTrees(target ~ ., train, test, main = "Crime Rate", decision.labels = c("Below Median", "Above Median"))

for (i in 1:6){
  p <- plot(fit, data = "test", tree=i)
  print(p)
}
```

```{r}
fit <- rpart(target ~ ., method="class", data=train)
rpart.plot(fit)
```


```{r}
get_high_nox <- function(x){
  if(x > 0.53){
    return("High")
  } else {
    return("Low")
  }
}

get_divider <- function(x, y){
  if(x < 10 & y < 0.6){
    return(1)
  } else {
    return(0)
  }
}

train <- train %>%
  rowwise() %>%
  mutate(high_nox = get_high_nox(nox),
         divider = get_divider(rad, nox))

test <- test %>%
  rowwise() %>%
  mutate(high_nox = get_high_nox(nox),
         divider = get_divider(rad, nox))
```

## Feature Engineering
```{r}

library(fuzzyjoin)



density_diff <- function(data,eval_data,var,target,new_column){

data[[paste0(var,"temp")]]<- data[[var]]

eval_data[[paste0(var,"temp")]]<- eval_data[[var]]
  
#standardize variable between 0 and 1
data[[var]] <- (data[[var]] - min(data[[var]]))/(max(data[[var]]) - min(data[[var]]))

eval_data[[var]] <- (eval_data[[var]] - min(eval_data[[var]]))/(max(eval_data[[var]]) - min(eval_data[[var]]))

## Calculate density estimates
g1 <- ggplot(data, aes(x=data[[var]], group=target, colour=target)) +
  geom_density(data = data) + xlim(min(data[[var]]), max(data[[var]]))
gg1 <- ggplot_build(g1)
# construct the dataset
x <- gg1$data[[1]]$x[gg1$data[[1]]$group == 1]
y1 <- gg1$data[[1]]$y[gg1$data[[1]]$group == 1]
y2 <- gg1$data[[1]]$y[gg1$data[[1]]$group == 2]
df2 <- data.frame(x = x, ymin = pmin(y1, y2), ymax = pmax(y1, y2), 
                  side=(y1<y2), ydiff = y2-y1)
##creating the second graph object
g3 <- ggplot(df2) +
  geom_line(aes(x = x, y = ydiff, colour = side)) +
  geom_area(aes(x = x, y = ydiff, fill = side, alpha = 0.4)) +
  guides(alpha = FALSE, fill = FALSE)


data$join <- data[[var]]
df2$join <- df2$x
temp <- difference_left_join(data,df2, by="join", max_dist =.001)
means <- aggregate(ydiff ~ temp$join.x, temp, mean)

colnames(means) <- c("std_var","density_diff")




new_data <- merge(means,data, by.x ="std_var", by.y =var)
new_data$std_var <- NULL
new_data$join <- NULL
new_data[new_column]<- new_data$density_diff
new_data$density_diff <- NULL
###################same thing with eval data ############################


eval_data$join <- eval_data[[var]]
df2$join <- df2$x
temp <- difference_left_join(eval_data,df2, by="join", max_dist =.001)
means <- aggregate(ydiff ~ temp$join.x, temp, mean)

##new stuff
colnames(means) <- c("std_var","density_diff")
#data["key"] <- (var - min(var))/(max(var) - min(var))


eval_data2 <- merge(means,eval_data, by.x ="std_var", by.y =var)
eval_data2$std_var <- NULL
eval_data2$join <- NULL
eval_data2[new_column]<- eval_data2$density_diff
eval_data2$density_diff <- NULL

eval_data2[[var]] <- eval_data2[[paste0(var,"temp")]]
eval_data2[[paste0(var,"temp")]]<- NULL

new_data[[var]] <- new_data[[paste0(var,"temp")]]
new_data[[paste0(var,"temp")]]<- NULL


mylist <- list(g1,g3,new_data,eval_data2)
names(mylist)<- c("dist","dist_diff","new_data_training","new_data_eval")
return(mylist)

}

```



```{r}
baseline <- glm(target ~ ., family = binomial(link = "logit"), train)
summary(baseline)
test$baseline <- ifelse(predict(baseline, test) < 0.5, 0, 1)
test$baseline_yhat <- predict(baseline, test)
confusionMatrix(factor(test$baseline), factor(test$target))
```


#Creating Some New Variables

```{r}
str(train)
str(test)
## try it out 
train <- density_diff(train,test,"nox",train$target,"nox_density")$new_data_training
train <- density_diff(train,test,"age",train$target,"age_density")$new_data_training 
train <- density_diff(train,test,"indus",train$target,"indus_density")$new_data_training
train <- density_diff(train,test,"dis",train$target,"dis_density")$new_data_training
train <- density_diff(train,test,"rad",train$target,"rad_density")$new_data_training
train <- density_diff(train,test,"tax",train$target,"tax_density")$new_data_training
train <- density_diff(train,test,"ptratio",train$target,"ptratio_density")$new_data_training
train <- density_diff(train,test,"lstat",train$target,"lstat_density")$new_data_training
train <- density_diff(train,test,"medv",train$target,"medv_density")$new_data_training
train <- density_diff(train,test,"rm",train$target,"rm_density")$new_data_training
train <- density_diff(train,test,"zn",train$target,"zn_density")$new_data_training

test<- density_diff(train,test,"nox",train$target,"nox_density")$new_data_eval
test<- density_diff(train,test,"age",train$target,"age_density")$new_data_eval
test<- density_diff(train,test,"indus",train$target,"indus_density")$new_data_eval
test<- density_diff(train,test,"dis",train$target,"dis_density")$new_data_eval
test<- density_diff(train,test,"rad",train$target,"rad_density")$new_data_eval
test<- density_diff(train,test,"tax",train$target,"tax_density")$new_data_eval
test<- density_diff(train,test,"ptratio",train$target,"ptratio_density")$new_data_eval
test<- density_diff(train,test,"lstat",train$target,"lstat_density")$new_data_eval
test<- density_diff(train,test,"medv",train$target,"medv_density")$new_data_eval
test<- density_diff(train,test,"rm",train$target,"rm_density")$new_data_eval
test<- density_diff(train,test,"zn",train$target,"zn_density")$new_data_eval

str(train)
str(test)


str(train)
str(test)



```



## Model Building

Applying occam's razor we will create a baseline that only has one predictor.  Any model will have to out preform this simplest model.

```{r}
baseline <- glm(target ~ nox, family = binomial(link = "logit"), train)
summary(baseline)
test$baseline <- ifelse(predict(baseline, test) < 0.5, 0, 1)
test$baseline_yhat <- predict(baseline, test)
confusionMatrix(factor(test$baseline), factor(test$target))
```


```{r}
model1 <- glm(target ~ high_nox + indus, family = binomial(link = "logit"), train)
summary(model1)
test$model1 <- ifelse(predict(model1, test) < 0.5, 0, 1)
test$model1_yhat <- predict(model1, test)
confusionMatrix(factor(test$model1), factor(test$target))
```


```{r}
model2 <- glm(target ~ divider, family = binomial(link = "logit"), train)
summary(model2)
test$model2 <- ifelse(predict(model2, test) < 0.5, 0, 1)
test$model2_yhat <- predict(model2, test)
confusionMatrix(factor(test$model2), factor(test$target))
```

```{r}

density_models <- glm(target ~ nox_density +indus_density+age_density+dis_density+rad_density+tax_density+ptratio_density+lstat_density+medv_density, family = binomial(link = "logit"), train)
summary(density_models)
test$density_models <- ifelse(predict(density_models, test) < 0.5, 0, 1)
test$density_models_yhat <- predict(density_models, test)
confusionMatrix(factor(test$density_models), factor(test$target))
```




## Model Selection

## pROC Output

```{r}
library(pROC)
par(pty = "s")
roc(test[["target"]], test[["density_models_yhat"]], plot = TRUE, legacy.axes = TRUE, print.auc = TRUE)
par(pty = "m")


library(pROC)
par(pty = "s")
roc(test[["target"]], test[["model2_yhat"]], plot = TRUE, legacy.axes = TRUE, print.auc = TRUE)
par(pty = "m")


library(pROC)
par(pty = "s")
roc(test[["target"]], test[["model1_yhat"]], plot = TRUE, legacy.axes = TRUE, print.auc = TRUE)
par(pty = "m")
```
