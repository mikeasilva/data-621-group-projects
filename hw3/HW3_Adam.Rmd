---
title: "DATA 621 Homework #3"
author: "Critical Thinking Group 3"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    highlight: kate
    toc_depth: 3
    code_folding: "hide"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment=NA, message=FALSE, warning=FALSE)
library(tidyverse)
library(kableExtra)
library(corrplot)
library(caret)
library(Amelia)
# Thank you Stack Overflow!
# A Prefix nulling hook.

# Make sure to keep the default for normal processing.
default_output_hook <- knitr::knit_hooks$get("output")

# Output hooks handle normal R console output.
knitr::knit_hooks$set( output = function(x, options) {

  comment <- knitr::opts_current$get("comment")
  if( is.na(comment) ) comment <- ""
  can_null <- grepl( paste0( comment, "\\s*\\[\\d?\\]" ),
                     x, perl = TRUE)
  do_null <- isTRUE( knitr::opts_current$get("null_prefix") )
  if( can_null && do_null ) {
    # By default R print output aligns at the right brace.
    align_index <- regexpr( "\\]", x )[1] - 1
    # Two cases: start or newline
    re <- paste0( "^.{", align_index, "}\\]")
    rep <- comment
    x <- gsub( re, rep,  x )
    re <- paste0( "\\\n.{", align_index, "}\\]")
    rep <- paste0( "\n", comment )
    x <- gsub( re, rep,  x )
  }

  default_output_hook( x, options )

})
knitr::opts_template$set("kill_prefix"=list(comment=NA, null_prefix=TRUE))
```

```{r}
df <- read.csv('data/crime-training-data_modified.csv')
evaluation <- read.csv("data/crime-evaluation-data_modified.csv")
```

## Data Exploration

### Are There Missing Values?

```{r}
missmap(df, main = "Missing vs Observed Values")
```

It looks like we have a complete data set.  No need to impute values.

### Splitting the Data

```{r}
set.seed(42)
train_index <- createDataPartition(df$target, p = .7, list = FALSE, times = 1)
train <- df[train_index,]
test <- df[-train_index,]
```


### Exploratory Data Analysis


```{r}
train %>% 
  cor(.) %>%
  corrplot(., method = "color", type = "upper", tl.col = "black", diag = FALSE)
```

The following show the how predictors are distributed between the areas with crime rates higher than the median *blue) and below the median (red).  What we are looking for is variables that could split data into two groups.

```{r}
for (var in names(train)){
  if(var != "target"){
    plot_df <- train
    plot_df$x <- plot_df[,var]
    p <- ggplot(plot_df, aes(x, color = factor(target))) +
      geom_density() +
      theme_light() +
      ggtitle(var) +
      scale_color_brewer(palette = "Set1") +
      theme(legend.position = "none",
            axis.title.y = element_blank(),
            axis.title.x = element_blank())
    print(p)
  }
}
```

NOX seems to be the best variable to divide the data into the two groups

## Data Preparation

## Model Building

Applying occam's razor we will create a baseline that only has one predictor.  Any model will have to out preform this simplest model.

```{r}
baseline <- glm(target ~ nox, family = binomial(link = "logit"), train)
summary(baseline)
test$baseline <- ifelse(predict.glm(baseline, test,"response") >= 0.5,1,0)
cm <- confusionMatrix(factor(test$baseline2), factor(test$target),"1")
results <- tibble(model = "baseline",predictors = 1,F1 = cm$byClass[7],
                  deviance=baseline$deviance, 
                  r2 = 1 - baseline$deviance/baseline$null.deviance,
                  aic=baseline$aic)
cm
```

Our baseline model is ok with an F1 score of `r cm$byClass[7]`.

Now we will try adding all other variables and work backwards to eliminate non-significant predictors:

```{r}
fullmodel <- glm(target ~ nox + dis, family = binomial(link = "logit"), train)
summary(fullmodel)
test$fullmodel <- ifelse(predict.glm(fullmodel, test,"response") < 0.5, 0, 1)
cm <- confusionMatrix(factor(test$fullmodel), factor(test$target),"1")
results <- rbind(results,tibble(model = "fullmodel",
                                predictors = 12,F1 = cm$byClass[7],
                                deviance=fullmodel$deviance, 
                                r2 = 1-fullmodel$deviance/fullmodel$null.deviance,
                                aic=fullmodel$aic))
cm
```

The full model has an F1 score is `r cm$byClass[7]`, which is a bit higher than before. However, the new variable doesn't seem to be significant.

After some backward elimination of non-significant predictor variables, we arrive at the following model: 

```{r}
model1 <- glm(target ~ . -tax -rm -chas - age -zn -indus, 
              family = binomial(link = "logit"), 
              train)
summary(model1)
test$model1 <- ifelse(predict.glm(model1, test,"response") < 0.5, 0, 1)
cm <- confusionMatrix(factor(test$model1), factor(test$target),"1")
results <- rbind(results,tibble(model = "model1",
                                predictors = 6,F1 = cm$byClass[7],
                                deviance=model1$deviance, 
                                r2 = 1-model1$deviance/model1$null.deviance,
                                aic=model1$aic))
cm
```

Looking at our models together, `model1` looks like the best so far based on highest F1 score and lowest deviance & AIC.

```{r}
kable(results)
```


## Model Selection

```{r}
library(FFTrees)

test <- test[names(train)]

fit <- FFTrees(target ~ ., train, test, main = "Crime Rate", decision.labels = c("Below Median", "Above Median"))

for (i in 1:6){
  p <- plot(fit, data = "test", tree=i)
  print(p)
}

```
